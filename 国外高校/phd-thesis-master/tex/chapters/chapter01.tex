\chapter{Introduction}\label{ch:introduction}

In various fields of science, technology and humanities, as in biology,
meteorology, medicine or finance to cite a few, experts aim at predicting a
phenomenon based on past observations or measurements. For instance,
meteorologists try to forecast the weather for the next days from the climatic
conditions of the previous days. In medicine, practitioners collect measurements
and information such as blood pressure, age or history for diagnosing the
condition of incoming patients. Similarly, in chemistry, compounds are analyzed
using mass spectrometry measurements in order to determine whether they contain
a given type of molecules or atoms. In all of these cases, the goal is the
prediction of a response variable based on a set of observed predictor
variables.

For centuries, scientists have addressed such problems by deriving theoretical
frameworks from first principles or have accumulated knowledge in order to
model, analyze and understand the pheno\-menon under study. For example,
practitioners know from past experience that elderly heart attack patients with
low blood pressure are generally high risk. Similarly, meteorologists know from
elementary climate models that one hot, high pollution day is likely to be
followed by another. For an increasing number of problems however, standard
approaches start showing their limits. For example, identifying the genetic
risk factors for heart disease, where knowledge is still very sparse, is nearly
impractical for the cognitive abilities of humans given the high complexity and
intricacy of interactions that exist between genes. Likewise, for very
fine-grained meteorological forecasts, a large number of variables need to be taken
into account, which quickly goes beyond the capabilities of experts to put them
all into a system of equations. To break this cognitive barrier and further
advance science, machines of increasing speed and capacity have been built and
designed since the mid-twentieth century to assist humans in their
calculations. Amazingly however, alongside this progress in terms of hardware,
developments in theoretical computer science, artificial intelligence and
statistics have made machines to become more than calculators. Recent advances
have made them experts of their own kind, capable to learn from data and to
uncover by themselves the predictive structure of problems. Techniques and
algorithms that have stemmed from the field of {\it machine learning} have
indeed now become a powerful tool for the analysis of complex and large data,
successfully assisting scientists in numerous breakthroughs of various fields
of science and technology. Public and famous examples include the use of
boosted decision trees in the statistical analysis that led to the detection of
the Higgs boson at CERN~\citep{chatrchyan:2012}, the use of random forests for
human pose detection in the Microsoft Kinect~\citep{criminisi:2013} or the
implementation of various machine learning techniques for building the IBM
Watson system~\citep{ferrucci:2010}, capable to compete at the human champion
level on the American TV quiz show Jeopardy.

Formally, machine learning can be defined as the study of systems that can
learn from data without being explicitly programmed. According to
\citet{mitchell:1997}, a computer program is said to learn from data, with
respect to some class of tasks and performance measure if its performance at
those tasks improves with data. In particular, machine learning provides
algorithms that are able to solve classification or regression tasks, hence
bringing now automated procedures for the prediction of a phenomenon based on
past observations. However, the goal of machine learning is not only to produce
algorithms making accurate predictions, it is also to provide insights on the
predictive structure of the data~\citep{breiman:1984}. If we are aiming at the
latter, then our goal is to understand what variables or interactions of
variables drive the phenomenon. For practitioners, which are not experts in
machine learning, interpretability is indeed often as important as prediction
accuracy. It allows for a better understanding of the phenomenon under study, a
finer exploration of the data and an easier self-appropriation of the results.
By contrast, when an algorithm is used as a black box, yielding results
seemingly out of nowhere, it may indeed be difficult to trust or accept if it
cannot be understood how and why the procedure came to them. Unfortunately, the
current state-of-the-art in machine learning often makes it difficult for
non-experts to understand and interpret the results of an algorithm. While
considerable efforts have been put to improve their prediction accuracy, it is
still not clearly understood what makes machine learning algorithms truly work,
and under what assumptions. Likewise, few of them actually provide clear and
insightful explanations about the results they generate.

In this context, the goal of this thesis is to provide a comprehensive and
self-contained analysis of a class of algorithms known as decision
trees~\citep{breiman:1984} and random forests~\citep{breiman:2001}. While these
methods have proven to be a robust, accurate and successful tool for solving
countless of machine learning tasks, including classification, regression,
density estimation, manifold learning or semi-supervised
learning~\citep{criminisi:2013}, there remain many gray areas in their
understanding:
\begin{enumerate}
\item First, the theoretical properties and statistical mechanisms that drive
the algorithm are still not clearly and entirely understood. Random forests
indeed evolved from empirical successes rather than from a sound
theory. As such, various parts of the algorithm remain heuristic rather than
theoretically motivated. For example, preliminary
results have proven the consistency of simplified to very close variants of
random forests, but consistency of the original algorithm remains unproven
in a general setting.
\item Second, while the construction process of a single decision tree can
easily be described within half a page, implementing this algorithm properly
and efficiently remains a challenging task involving issues that are easily
overlooked. Unfortunately, implementation details are often omitted in the
scientific literature and can often only be found by diving into
(unequally documented) existing software implementations. As far as we know,
there is indeed no comprehensive survey covering the implementation details of
random forests, nor with their respective effects in terms of runtime and space
complexity or learning ability.
\item Third, interpreting the resulting model remains a difficult task,
for which even machine learning experts still fail at finely analyzing and
uncovering the precise predictive structure learned by the procedure.
In particular, despite their extensive use in a wide range of applications, little
is still known regarding variable importance measures computed by random forests.
Empirical evidence suggests that they are appropriate for identifying
relevant variables, but their statistical mechanisms and properties are
still far from being understood.
\end{enumerate}
All throughout this dissertation, our objective is therefore to call into
question each and every part of the random forests methodology,  both from a
theoretical and practical point of view. Accordingly, this work aims at
revisiting decision trees and random forests to hopefully shed new light on
their learning capabilities, inner workings and interpretability.

\section{Outline and contributions}

Part~\textsc{\ref{part:1}} of this manuscript is first dedicated to a thorough
treatment of decision trees and forests of randomized trees. We begin in
Chapter~\ref{ch:background} by outlining fundamental concepts of machine
learning, and then proceed in Chapters~\ref{ch:cart} and \ref{ch:forest} with a
comprehensive review of the algorithms at the core of decision trees and random
forests. We discuss the learning capabilities of these models and carefully
study all parts of the algorithm and their complementary effects. In particular,
Chapter~\ref{ch:forest} includes original contributions on the bias-variance
analysis of ensemble methods, highlighting how randomization can help improve
performance. Chapter~\ref{ch:complexity} concludes this first part with an
original space and time complexity analysis of random forests (and their
variants), along with an in-depth discussion of implementation details,
as contributed within the open source Scikit-Learn library.
Overall, Part~\textsc{\ref{part:1}} therefore presents a comprehensive review
of previous work on random forests, including some original contributions
both from a theoretical and practical point of view.

Part~\textsc{\ref{part:2}} analyzes and discusses the interpretability of
random forests. In Chapter~\ref{ch:importances}, we study variable importances
as computed with a forest of randomized trees and study how these scores can be
interpreted in order to reveal the underlying predictive structure learned from
the data. In particular, we derive a theoretical framework from which we prove
theoretical and practical properties of variable importances. In
Chapter~\ref{ch:applications}, we then exploit this framework to further study
variable importances as derived from actual random forests and present
successful applications of variable importance measures.
Part~\textsc{\ref{part:2}} constitutes the main contributions of this
dissertation.

Finally, Part~\textsc{\ref{part:3}} addresses limitations of random forests in
the context of large datasets. Through extensive experiments, we show in
Chapter~\ref{ch:random-patches} that subsampling strategies provides on par
performance while simultaneously lowering the memory requirements. This
chapter presents original work.

\section{Publications}

This dissertation summarizes several contributions to random forests
algorithms. Publications that have directly stemmed from this work include:

\begin{itemize}
\item \citep{geurts:2011} \textit{Learning to rank with extremely randomized trees},
Geurts Pierre and Louppe Gilles.
In JMLR: Workshop and Conference Proceedings, volume 14, 2011.

\item \citep{louppe:2012} \textit{Ensembles on random patches},
Louppe Gilles and Geurts Pierre.
In Machine Learning and Knowledge Discovery in Databases, pages 346--361. Springer, 2012.

\item \citep{louppe:2013} \textit{Understanding variable importances in forests of randomized trees},
Louppe Gilles, Wehenkel Louis, Sutera Antonio and Geurts Pierre.
In Advances in Neural Information Processing Systems, pages 431--439, 2013.

\item \citep{buitinck:2013} \textit{API design for machine learning software: experiences from the scikit-learn project},
Buitinck Lars, Louppe Gilles, Blondel Mathieu et al..
In ECML-PKDD 2013 Workshop: Languages for Data Mining and Machine Learning, 2013.

\item \citep{botta:2014} \textit{Exploiting SNP Correlations within Random Forest for Genome-Wide Association Studies},
Botta Vincent, Louppe Gilles, Geurts Pierre and Wehenkel Louis.
PloS one, 9(4):e93379, 2014.

\end{itemize}

During the course of this thesis, several fruitful collaborations have also
led to the following publications. These are not discussed within
this dissertation.

\begin{itemize}

\item \citep{louppe:2010} \textit{A zealous parallel gradient descent algorithm},
Louppe Gilles and Geurts Pierre.
In Learning on Cores, Clusters and Clouds workshop, NIPS, 2010

\item \citep{maree:2014} \textit{A hybrid human-computer approach for large-scale image-based measurements using web services and machine learning},
Mar{\'e}e Rapha{\"e}l, Rollus Loic, Stevens Benjamin et al.
Proceedings IEEE International Symposium on Biomedical Imaging, 2014.

\item \citep{amy:2014} \textit{Solar Energy Prediction: An International Contest to Initiate Interdisciplinary Research on Compelling Meteorological Problems},
Amy McGovern, David John Gagne II, Lucas Eustaquio et al., 2014. \textit{Submitted.}

\item \citep{sutera:2014} \textit{Simple connectome inference from partial correlation statistics in calcium imaging},
Antonio Sutera, Arnaud Joly, Vincent Francois-Lavet et al., 2014. \textit{Submitted.}

\end{itemize}
