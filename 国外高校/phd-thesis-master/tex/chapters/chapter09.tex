\chapter{Conclusions}\label{ch:conclusions}

By and large, machine learning remains an open field of research for which many
questions are still left unanswered, even regarding well-established methods.
In this dissertation, we have revisited decision trees and random forests,
consistently calling into question each and every part of these algorithms, in
order to shed new light on their learning capabilities, inner workings and
interpretability.

In Part~\textsc{\ref{part:1}} of this work, we laid out the decision trees and
random forests methodology in the context of classification and regression
tasks. Our treatment first considered the induction of individual decision
trees and put them into a unified and composable framework. In particular, our
analysis reviewed assignment rules, stopping criteria and splitting rules,
theoretically motivating their design and purpose whenever possible. We then
proceeded with a systematic study of randomized ensemble methods within the
bias-variance framework. We established that variance depends on the
correlation between individual tree predictions, thereby showing why
randomization acts as a mechanism for reducing the generalization error of an
ensemble.  Random forest and its variants were then presented within the
framework previously introduced, and their properties and features discussed
and reviewed. Our contributions followed with an original time and space
complexity analysis of random forests, hence showing their good computational
performance and scalability to larger problems. Finally, the first part of this
work concluded with an in-depth discussion of implementation details of random
forests, highlighting and discussing considerations that are critical, yet
easily overlooked, for guaranteeing good computational performance. While not
directly apparent within this manuscript, this discussion also underlined our
contributions in terms of software, within the open source Sckit-Learn library.
As open science and reproducibility concerns are gaining momentum, we indeed
believe that good quality software should be an integrative part, acknowledged
for its own value and impact, of any modern scientific research activity.

Part~\textsc{\ref{part:2}} of this dissertation analyzed and discussed the
interpretability of random forests in the eyes of variable importance measures.
The core of our contributions rests in the theoretical characterization of the
Mean Decrease of Impurity variable importance measure, from which we have then
proved and derived some of its properties in the case of multiway totally
randomized trees and in asymptotic conditions. In particular, we have shown
that variable importances offer a three-level decomposition of the information
jointly provided by the input variables about the output, accounting for all
possible interaction terms in a fair and exhaustive way. More interestingly, we
have also shown that variable importances only depend on relevant variables and
that the importance of irrelevant variables is strictly equal to zero, thereby
making importances a sound and appropriate criterion for assessing the
usefulness of variables. In consequence of this work, our analysis then
demonstrated that variable importances as computed from non-totally randomized
trees (e.g., standard Random Forest or Extremely Randomized Trees) suffer from
a combination of defects, due to masking effects, misestimations of node
impurity or due to the binary structure of decision trees. Overall, we believe
that our analysis should bring helpful insights in a wide range of
applications, by shedding new light on variable importances. In particular, we
advise to complement their interpretation and analysis with a systematic
decomposition of their terms, in order to better understand why variables are
(or are not) important.

This preliminary work unveils various directions of future work, both from a
theoretical and practical point of view. To our belief, the most interesting
theoretical open question would be the characterization of the distribution of
variable importances in the finite setting. Such a characterization would
indeed allow to more reliably distinguish irrelevant variables (whose
importances are positive in the finite case) from relevant variables. Another
interesting direction of future work would be to derive a proper
characterization of variable importances in the case of binary trees -- even if we
believe, as pointed out earlier, that variable importances derived from such
ensembles may in fact not be as appropriate as desired. From a more practical
point of view, this study also calls for a re-analysis of previous empirical
studies. We indeed believe that variable importances along with their
decomposition should yield new insights in many cases, providing a better
understanding of the interactions between the input variables and the output,
but also between the input variables themselves. Again, we recommend multiway
totally randomized trees to mitigate sources of bias as much as possible.

Finally, Part~\textsc{\ref{part:3}} addressed limitations of random forests in
the context of large datasets. Through extensive experiments, we have shown
that subsampling either samples, features or both simultaneously provides on
par performance while lowering at the same time the memory requirements.
Overall this paradigm highlights an intriguing practical fact: there is often
no need to build single models over immensely large datasets. Good performance
can often more simply be achieved by building models on small random parts of the
data and then combining them all in an ensemble, thereby avoiding all practical and
computational burdens of making large data fit into memory. Again, this work
raises interesting questions of further work. From a theoretical point of view,
one would be to identify the statistical properties in the learning problem
that are necessary for guaranteeing subsampling strategies to work. In
particular, in which cases is it better to subsample examples rather than
features? From a more practical point of view, other directions of research
also include the study of smarter sampling strategies or the empirical
verification that conclusions extend to non tree-based methods.

Overall, this thesis calls for a permanent re-assessment of machine learning
methods and algorithms. It is only through a better understanding of their
mechanisms that algorithms will advance in a consistent and reliable way.
Always seek for the what and why. In conclusion, machine learning should not be
considered as a black-box tool, but as a methodology, with a rational thought
process that is entirely dependent on the problem we are trying to solve.
