% Notations ====================================================================

\chapter{Notations}

\begin{tabularx}{\textwidth}{ l X }
${\cal A}$ & A supervised learning algorithm \dotfill  \pageref{ntn:A}\\
${\cal A}(\theta, {\cal L})$ & The model $\varphi_{\cal L}$ produced by algorithm ${\cal A}$ over ${\cal L}$ and hyper-parameters $\theta$ \dotfill  \pageref{ntn:A-func}\\
$\alpha_s$ & The proportion of samples in a random patch \dotfill  \pageref{ntn:alpha_s}\\
$\alpha_f$ & The proportion of features in a random patch \dotfill  \pageref{ntn:alpha_f}\\
$b_l$ & The $l$-th value of a categorical variable \dotfill  \pageref{ntn:b_l}\\
$B$ & A subset $B \subseteq V$ of variables \dotfill  \pageref{ntn:B}\\
$c_k$ & The $k$-th class \dotfill  \pageref{ntn:c_k}\\
$C^k_p$ & The number of $k$-combinations from a set of $p$ elements \dotfill  \pageref{ntn:C_k_p}\\
$C(N)$ & The time complexity for splitting $N$ samples \dotfill  \pageref{ntn:cN}\\
$\mathbb{E}$ & Expectation \dotfill \\
$\overline{E}(\varphi_{\cal L}, {\cal L}^\prime)$ & The average prediction error of $\varphi_{\cal L}$ over ${\cal L}^\prime$ \dotfill  \pageref{ntn:E_bar}\\
$Err(\varphi_{\cal L})$ & The generalization error of $\varphi_{\cal L}$ \dotfill  \pageref{eqn:generalization-error}, \pageref{eqn:4:generalization-error}\\
%$\widehat{Err}^\text{train}(\varphi_{\cal L})$ & The resubstitution estimate or training sample estimate of the generalization error of $\varphi_{\cal L}$ \dotfill  \pageref{eqn:training-error}\\
%$\widehat{Err}^\text{test}(\varphi_{\cal L})$ & The test sample estimate of the generalization error of $\varphi_{\cal L}$ \dotfill  \pageref{eqn:test-error}\\
%$\widehat{Err}^\text{CV}(\varphi_{\cal L})$ & The cross-validation  estimate of the generalization error of $\varphi_{\cal L}$ \dotfill  \pageref{eqn:cv-error}\\
%$\widehat{Err}^\text{OOB}(\psi_{\cal L})$ & The out-of-bag estimate of the generalization error of $\psi_{\cal L}$ \dotfill  \pageref{eqn:oob-error}\\
$H(X)$ & The Shannon entropy of $X$ \dotfill  \pageref{eqn:6:entropy}\\
$H(X|Y)$ & The Shannon entropy of $X$ conditional to $Y$\dotfill  \pageref{eqn:6:entropy-cond}\\
${\cal H}$ & The space of candidate models \dotfill  \pageref{ntn:H}\\
$i(t)$ & The impurity of node $t$ \dotfill  \pageref{ntn:i_t}, \pageref{ntn:i_t2}\\
$i_R(t)$ & The impurity of node $t$ based on the local resubstitution estimate \dotfill \pageref{eqn:impurity:error},~\pageref{eqn:impurity:variance}\\
$i_H(t)$ & The entropy impurity of node $t$ \dotfill  \pageref{eqn:impurity:shannon}\\
$i_G(t)$ & The Gini impurity of node $t$ \dotfill  \pageref{eqn:impurity:gini}\\
$\Delta i(s, t)$ & The impurity decrease of the split $s$ at node $t$ \dotfill  \pageref{def:impurity-decrease}\\
$I(X;Y)$ & The mutual information between $X$ and $Y$ \dotfill  \pageref{eqn:6:mi}\\
$\text{Imp}(X_j)$ & The variable importance of $X_j$ \dotfill \pageref{eq:mdi}, \pageref{eq:mda}\\
$J$ & The number of classes \dotfill  \pageref{ntn:J}\\
$K$ & The number of folds in cross-validation \dotfill  \pageref{ntn:K-cv} \newline The number of input variables drawn at each node for finding a split \dotfill \pageref{ntn:K-split} \\
$K(\mathbf{x}_i, \mathbf{x}_j)$ & The kernel of $\mathbf{x}_i$ and $\mathbf{x}_j$ \dotfill \pageref{ntn:kernel}, \pageref{ntn:kernel2}\\
$L$ & A loss function \dotfill  \pageref{ntn:L}\newline The number of values of a categorical variable \dotfill \pageref{ntn:L2}\\
${\cal L}$ & A learning set $(\mathbf{X}, \mathbf{y})$ \dotfill  \pageref{ntn:learning-set}\\
${\cal L}^m$ & The $m$-th bootstrap replicate of ${\cal L}$ \dotfill  \pageref{ntn:L_m}\\
${\cal L}_t$ & The subset of node samples falling into node $t$ \dotfill  \pageref{ntn:L_t}\\
$M$ & The number of base models in an ensemble \dotfill  \pageref{ntn:M}\\
$\mu_{{\cal L},\theta_m}(\mathbf{x})$ & The mean prediction at $X = \mathbf{x}$ of $\varphi_{{\cal L},\theta_m}$ \dotfill \pageref{eqn:4:mu} \\
$N$ & The number of input samples \dotfill  \pageref{ntn:N}\\
$N_t$ & The number of node samples in node $t$ \dotfill  \pageref{ntn:N_t}\\
$N_{ct}$ & The number of node samples of class $c$ in node $t$ \dotfill  \pageref{ntn:N_ct}\\
$\Omega$ & The universe, or population, from which cases are sampled \dotfill  \pageref{ntn:omega}\\
$p$ & The number of input variables \dotfill  \pageref{ntn:p}\\
$p_L$ & The proportion of node samples going to $t_L$ \dotfill  \pageref{ntn:p_L}\\
$p_R$ & The proportion of node samples going to $t_R$ \dotfill  \pageref{ntn:p_R}\\
$p(t)$ & The estimated probability $p(X \in {\cal X}_t)=\tfrac{N_t}{N}$ \dotfill  \pageref{ntn:p_t}\\
$p(c|t)$ & The empirical probability estimate $p(Y=c | X \in {\cal X}_t)=\tfrac{N_{ct}}{N_t}$ of class $c$ at node $t$ \dotfill  \pageref{ntn:p_ct}\\
$\widehat{p}_{\cal L}$ & An empirical probability estimate computed from the learning set ${\cal L}$\dotfill  \pageref{eqn:4:proba-estimates}\\
$P(X,Y)$ & The joint probability distribution of the input variables $X=(X_1,\dots,X_p)$ and the output variable $Y$ \dotfill  \pageref{ntn:P_XY}\\
${\cal P}_k(V)$ & The set of subsets of $V$ of size $k$ \dotfill  \pageref{ntn:P_k}\\
$\varphi$ & A model or function ${\cal X} \mapsto {\cal Y}$ \dotfill  \pageref{ntn:varphi}\newline A single decision tree \dotfill  \pageref{ntn:tree}\\
$\widetilde{\varphi}$ & The set of terminal nodes in $\varphi$ \dotfill  \pageref{ntn:varphi-leafs}\\
$\varphi(\mathbf{x})$ & The prediction of $\varphi$ for the sample $\mathbf{x}$ \dotfill  \pageref{ntn:varphi-x}\\
$\varphi_{\cal L}$ & A model built from ${\cal L}$ \dotfill  \pageref{ntn:varphi-L}\\
$\varphi_{{\cal L},\theta}$ & A model built from ${\cal L}$ with random seed $\theta$ \dotfill  \pageref{ntn:varphi-Ltheta}\\
$\varphi_B$ & A Bayes model \dotfill  \pageref{ntn:varphi-B}\\
$\psi_{{\cal L},\theta_1,\dots,\theta_M}$ & An ensemble of $M$ models built from ${\cal L}$ and random seeds $\theta_1, \dots, \theta_M$ \dotfill \pageref{ntn:psi} \\
${\cal Q}$ & A set ${\cal Q} \subseteq {\cal S}$ of splits of restricted structure \dotfill \pageref{ntn:Q}, \pageref{ntn:Q2}\\
${\cal Q}(X_j)$ & The set ${\cal Q}(X_j) \subseteq {\cal Q}$ of univariate binary splits that can be defined on variable $X_j$ \dotfill \pageref{eqn:q:ordered}, \pageref{eqn:q:categorical-cart}\\
$\rho(\mathbf{x})$ & The correlation coefficient between the predictions at $X=\mathbf{x}$ of two randomized models \dotfill \pageref{eqn:4:correlation} \\
$s$ & A split \dotfill  \pageref{ntn:s}, \pageref{ntn:s2}\\
$s^*$ & The best split \dotfill  \pageref{ntn:s-star}, \pageref{eqn:best-best-split}\\
$s^*_j$ & The best binary split defined on variable $X_j$\dotfill  \pageref{ntn:s-star}, \pageref{eqn:best-split-single}\\
$s_j^v$ & The binary split $(\{\mathbf{x}|x_j \leq v\}, \{\mathbf{x} > v\})$ defined on variable $X_j$ with discretization threshold $v$ \dotfill  \pageref{ntn:s_jv}\\
$s_t$ & The split labeling node $t$ \dotfill  \pageref{ntn:s_t}\\
$\tilde{s}^j_t$ & The best surrogate split for $s_t$ defined from $X_j$ \dotfill \pageref{ntn:s-surrogate}\\
${\cal S}$ & The set of all possible splits $s$ \dotfill  \pageref{ntn:S-all}\\
$\sigma^2_{{\cal L},\theta_m}(\mathbf{x})$ & The prediction variance at $X = \mathbf{x}$ of $\varphi_{{\cal L},\theta_m}$ \dotfill \pageref{eqn:4:sigma} \\
$t$ & A node in a decision tree \dotfill  \pageref{ntn:node}\\
$t_L$ & The left child of node $t$ \dotfill \pageref{ntn:t_L}, \pageref{ntn:t_L2}\\
$t_R$ & The right child of node $t$ \dotfill \pageref{ntn:t_R}, \pageref{ntn:t_R2}\\
$\theta$ & A vector of hyper-parameter values \dotfill  \pageref{ntn:theta}\newline A random seed \dotfill \pageref{ntn:theta-seed}\\
$\theta^*$ & The optimal hyper-parameters \dotfill  \pageref{ntn:theta-star}\\
$\widehat{\theta}^*$ & The approximately optimal hyper-parameters \dotfill  \pageref{ntn:theta-star-approx}\\
$\theta_m$ & The seed of the $m$-th model in an ensemble \dotfill  \pageref{ntn:theta-seed-m}\\
$v$ & A discretization threshold in a binary split \dotfill  \pageref{ntn:v}\\
$v_k$ & The $k$-th value of an ordered variable, when node samples are in sorted order \dotfill  \pageref{ntn:v_k}\\
$v_k^\prime$ & The mid-cut point between $v_k$ and $v_{k+1}$ \dotfill  \pageref{ntn:v_k_prime}\\
$V$ & The set $\{X_1, \dots, X_p\}$ of input variables \dotfill  \pageref{ntn:V}\\
$V^{-j}$ & $V \setminus \{X_j\}$ \dotfill  \pageref{ntn:V-j}\\
$\mathbb{V}$ & Variance \dotfill \\
$\textbf{x}$ & A case, sample or input vector $(x_1, \dots, x_p)$ \dotfill  \pageref{ntn:sample-x}\\
$\textbf{x}_i$ & The $i$-th input sample in ${\cal L}$ \dotfill  \pageref{ntn:sample-x_i}\\
$x_j$ & The value of variable $X_j$ for the sample $\textbf{x}$ \dotfill  \pageref{ntn:value-x_j}\\
$\textbf{X}$ & The $N\times p$ matrix representing the values of all $N$ samples for all $p$ input variables \dotfill  \pageref{ntn:matrix-X}\\
$X_j$ & The $j$-th input variable or feature \dotfill  \pageref{ntn:var-X_j}, \pageref{ntn:var-X_j2}\\
$X$ & The random vector $(X_1,\dots,X_p)$ \dotfill  \pageref{ntn:vector-X}\\
${\cal X}_j$ & The domain or space of variable $X_j$ \dotfill  \pageref{ntn:space-X_j}\\
${\cal X}$ & The input space ${\cal X}_1 \times \dots \times {\cal X}_p$ \dotfill  \pageref{ntn:space-X}\\
${\cal X}_t$ & The subspace ${\cal X}_t \subseteq {\cal X}$ represented by node $t$ \dotfill  \pageref{ntn:node-space}\\
$y$ & A value of the output variable $Y$ \dotfill  \pageref{ntn:value-y}\\
$\widehat{y}_t$ & The value labelling node $t$ \dotfill  \pageref{ntn:y_t}\\
$\widehat{y}_t^*$ & The optimal value labelling node $t$ \dotfill  \pageref{ntn:y_t-star}\\
$\mathbf{y}$ & The output values $(y_1,\dots,y_N)$ \dotfill  \pageref{ntn:vector-y}\\
$Y$ & The output or response variable $Y$ \dotfill  \pageref{ntn:var-Y}\\
${\cal Y}$ & The domain or space of variable $Y$ \dotfill  \pageref{ntn:space-Y}\\
\end{tabularx}
