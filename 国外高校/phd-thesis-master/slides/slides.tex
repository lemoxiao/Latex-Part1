\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{url}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{minted}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{eulervm}
\usepackage{auto-pst-pdf}
\usepackage{pst-plot}
\usepackage{multirow}


\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue}
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{sections/subsections in toc}[circle]
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{itemize subitem}[square]

\title{{\bf Understanding Random Forests}\\
From Theory to Practice}
\author{Gilles Louppe}
\institute{Université de Liège, Belgium}
\date{October 9, 2014}

\newcommand{\todo}[1]{\textcolor{red}{[TODO] #1}}

\definecolor{lightgreen}{rgb}{0.0,0.8,0.0}
\definecolor{lightblue}{rgb}{0.3,0.8,1.0}
\definecolor{lightred}{rgb}{0.874,0.180,0.105}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{lightgray}{rgb}{0.8,0.8,0.8}
\definecolor{shadecolor}{rgb}{0.9,0.9,0.9}
\newrgbcolor{mygreen}{.00 .5 .00}
\newrgbcolor{myyellow}{.6 .6 .00}

\DeclareMathOperator*{\argmax}{arg\,max}


\newrgbcolor{mygreen}{.00 .5 .00}
\newcommand{\X}[1]{\textcolor{blue}{#1}}
\newcommand{\y}[1]{\textcolor{red}{#1}}
\newcommand{\model}[1]{\textcolor{mygreen}{#1}}
\newcommand{\loss}[1]{\textcolor{lightblue}{#1}}

\begin{document}

\renewcommand{\inserttotalframenumber}{39}

% Title page ==================================================================

\begin{frame}
\titlepage
\end{frame}


% Motivation ==================================================================



\section{Motivation}

\begin{frame}{Motivation}

\begin{figure}
\vspace{-0.5cm}
\includegraphics[scale=0.4]{./figures/motivation.png}
\end{figure}

\end{frame}

\begin{frame}{Objective}

\begin{center}
From a set of {\bf \X{measurements}},

\vspace{1cm}
learn a {\bf \model{model}}
\vspace{1cm}

to predict and understand {\bf \y{a phenomenon}}.
\end{center}

\end{frame}

\begin{frame}{Running example}

\begin{columns}
\begin{column}{0.5\textwidth}

\begin{center}
From {\bf \X{physicochemical properties}} (alcohol, acidity, sulphates, ...),

\vspace{1cm}
learn a {\bf \model{model}}
\vspace{1cm}

to predict {\bf \y{wine taste preferences}} (from 0 to 10).

\end{center}

\end{column}
\begin{column}{0.5\textwidth}
  \begin{figure}
  \vspace{-0.5cm}
  \includegraphics[scale=0.6]{./figures/wine.jpg}
  \end{figure}
\end{column}
\end{columns}

\vspace{1cm}

{\footnotesize
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis,
{\it Modeling wine preferences by data mining from physicochemical properties},
2009.}

\end{frame}


% Supervised learning =========================================================

\begin{frame}
  \frametitle{Outline}
  %\tableofcontents
  \setbeamertemplate{enumerate items}[circle]
  \begin{enumerate}
  \item Motivation

  \vspace{0.5cm}

  \item Growing decision trees and random forests\\
    {\scriptsize Review of state-of-the-art, minor contributions}

\vspace{0.5cm}

  \item Interpreting random forests\\
    {\scriptsize Major contributions (Theory)}\\


\vspace{0.5cm}

  \item Implementing and accelerating random forests\\
    {\scriptsize Major contributions (Practice)}


\vspace{0.5cm}

  \item Conclusions
  \end{enumerate}
\end{frame}


\section{Growing decision trees and random forests}

\AtBeginSection[]
{
\begin{frame}
  \frametitle{Outline}
  \tableofcontents[currentsection]
  % Die Option [pausesections]
\end{frame}
}

\begin{frame}{Supervised learning}

\begin{itemize}
\item The \X{inputs} are random variables \X{$X = X_1$, ..., $X_p$};
\item The \y{output} is a random variable \y{$Y$}.
\end{itemize}

\begin{itemize}
\item Data comes as a finite learning set $${\cal L} = \{(\X{\mathbf{x}_i}, \y{y_i}) | i = 0, \dots, N-1 \},$$
where \X{$\mathbf{x}_i \in {\cal X} = {\cal X}_1 \times ... \times {\cal X}_p$} and \y{$y_i \in {\cal Y}$}
are randomly drawn from $P_{\X{X},\y{Y}}$.\\
\vspace{0.3cm}
E.g., $(\X{\mathbf{x}_i}, \y{y_i}) = ((\X{\text{color}=\text{red}}, \X{\text{alcohol}=12}, ...), \y{\text{score}=6})$
\end{itemize}

\begin{itemize}
\item The goal is to find a model $\model{\varphi_{\cal L}}: \X{{\cal X}} \mapsto \y{{\cal Y}}$ minimizing
$$
Err(\model{\varphi_{\cal L}}) = \mathbb{E}_{\X{X}, \y{Y}}\{ L(\y{Y}, \model{\varphi_{\cal L}}(\X{X})) \}.
$$
\end{itemize}

\end{frame}

\begin{frame}{Performance evaluation}

\begin{block}{Classification}
  \begin{itemize}
  \item Symbolic output (e.g., ${\cal Y} = \{ \text{yes}, \text{no} \}$)
  \item Zero-one loss
    $$L(Y, \varphi_{\cal L}(X)) = 1(Y \neq \varphi_{\cal L}(X))$$
  \end{itemize}
\end{block}

\begin{block}{Regression}
  \begin{itemize}
  \item Numerical output (e.g., ${\cal Y} = \mathbb{R}$)
  \item Squared error loss
    $$L(Y, \varphi_{\cal L}(X)) = (Y - \varphi_{\cal L}(X))^2$$
  \end{itemize}

\end{block}

\end{frame}


% Decision trees ==============================================================

\begin{frame}{Divide and conquer}
\begin{figure}
\only<1>{\includegraphics[scale=0.75]{./figures/tree-partition-c.pdf}}
\only<2>{\includegraphics[scale=0.75]{./figures/tree-partition-b.pdf}}
\only<3>{\includegraphics[scale=0.75]{./figures/tree-partition-a.pdf}}
\end{figure}
\end{frame}

\begin{frame}{Decision trees}
\begin{columns}
\begin{column}{0.3\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{./figures/tree-partition-d.pdf}
\end{figure}
\end{column}
\begin{column}{0.7\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{./figures/tree-simple.pdf}
\end{figure}
\end{column}
\end{columns}

\vspace{1cm}

$t \in \varphi$: nodes of the tree $\varphi$\\
$X_t$: split variable at $t$ \\
$v_t \in \mathbb{R}$: split threshold at $t$\\
$\varphi(\mathbf{x}) = \argmax_{c \in {\cal Y}} p(Y = c | X = \mathbf{x})$
\end{frame}

\begin{frame}[fragile]{Learning from data {\scriptsize (CART)}}

\begin{algorithmic}
\Function{BuildDecisionTree}{${\cal L}$}
    \State Create node $t$ from the learning sample ${\cal L}_t = {\cal L}$
    \If{the stopping criterion is met for $t$}
        \State $\widehat{y}_{t} =$ some constant value
    \Else
        \State Find the split on ${\cal L}_t$ that maximizes impurity decrease $$s^* = \argmax_{s \in {\cal Q}} \Delta i(s, t)$$
        \State Partition ${\cal L}_t$ into ${\cal L}_{t_L} \cup {\cal L}_{t_R}$ according to $s^*$
        \State $t_L = \Call{BuildDecisionTree}{{\cal L}_{L}}$
        \State $t_R = \Call{BuildDecisionTree}{{\cal L}_{R}}$
    \EndIf
    \State \Return $t$
\EndFunction
\end{algorithmic}

\end{frame}

\begin{frame}{Back to our example}
\begin{figure}
\includegraphics[scale=0.4]{./figures/tree-wine.pdf}
\end{figure}
\end{frame}

% Bias-variance ===============================================================

\begin{frame}{Bias-variance decomposition}

\begin{columns}
\begin{column}{0.65\textwidth}

{\bf Theorem.} For the squared error loss, the bias-variance decomposition of the expected
generalization error at $X=\mathbf{x}$ is
\begin{equation*}
\mathbb{E}_{\cal L} \{ Err(\varphi_{\cal L}(\mathbf{x})) \} = \text{noise}(\mathbf{x}) + \text{bias}^2(\mathbf{x}) + \text{var}(\mathbf{x})
\end{equation*}
where
\begin{align*}
\text{noise}(\mathbf{x}) &= Err(\varphi_B(\mathbf{x})), \\
\text{bias}^2(\mathbf{x}) &= (\varphi_B(\mathbf{x}) - \mathbb{E}_{\cal L} \{ \varphi_{\cal L}(\mathbf{x}) \} )^2, \\
\text{var}(\mathbf{x}) &= \mathbb{E}_{\cal L} \{ (\mathbb{E}_{\cal L} \{ \varphi_{\cal L}(\mathbf{x}) \} - \varphi_{\cal L}(\mathbf{x}))^2 \}.
\end{align*}

\end{column}
\begin{column}{0.35\textwidth}

\begin{figure}
\vspace{-1cm}
\hspace*{-0.4cm}\includegraphics[scale=0.3]{./figures/bias-variance.pdf}\\
\includegraphics[scale=0.4]{./figures/bias-variance-darts.jpg}
\end{figure}

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Diagnosing the generalization error of a decision tree}

\begin{itemize}
\item (Residual error: Lowest achievable error, independent of $\varphi_{\cal L}$.)
\item Bias: Decision trees usually have {\color{blue} low bias}.
\item Variance: They often suffer from {\color{red} high variance}.
\end{itemize}

\begin{itemize}
\item Solution: {\it Combine the predictions of several randomized trees into a single model.}
\end{itemize}

\end{frame}


% Forests =====================================================================

\begin{frame}{Random forests}
\begin{figure}
    \includegraphics[scale=0.5]{./figures/forest.pdf}
\end{figure}

Randomization

\vspace{0.1cm}

{\scriptsize
\begin{tabular}{lll}

\textbullet\hspace*{0.1cm} Bootstrap samples & \multirow{2}{*}{{\LARGE \}} {\color{blue} Random Forests}} & \\
\textbullet\hspace*{0.1cm}  Random selection of $K \leq p$ split variables              &&  \multirow{2}{*}{{\LARGE \}} {\color{blue} Extra-Trees}}                                    \\
\textbullet\hspace*{0.1cm}  Random selection of the threshold           &                                          \\
\end{tabular}}

% {\scriptsize
% \begin{itemize}
% \item Bootstrap samples
% \item Random selection of $K \leq p$ candidate split variables
% \item Random selection of the threshold
% \end{itemize}}
\end{frame}



\begin{frame}{Bias-variance decomposition (cont.)}

{\bf Theorem.}
For the squared error loss, the bias-variance decomposition of the expected
generalization error $\mathbb{E}_{\cal L} \{ Err( \psi_{{\cal L},\theta_1,\dots,\theta_M}(\mathbf{x}))
\}$ at $X=\mathbf{x}$ of an ensemble of $M$ randomized models $\varphi_{{\cal L},\theta_m}$ is
\begin{equation*}
\mathbb{E}_{\cal L} \{ Err(\psi_{{\cal L},\theta_1,\dots,\theta_M}(\mathbf{x})) \} = \text{noise}(\mathbf{x}) + \text{bias}^2(\mathbf{x}) + \text{var}(\mathbf{x}),
\end{equation*}
where
\begin{align*}
\text{noise}(\mathbf{x}) &= Err(\varphi_B(\mathbf{x})), \\
\text{bias}^2(\mathbf{x}) &= (\varphi_B(\mathbf{x}) - \mathbb{E}_{{\cal L},\theta} \{ \varphi_{{\cal L},\theta}(\mathbf{x}) \} )^2, \\
\text{var}(\mathbf{x}) &= \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x}) + \frac{1 - \rho(\mathbf{x})}{M} \sigma^2_{{\cal L},\theta}(\mathbf{x}).
\end{align*}

and where $\rho(\mathbf{x})$ is the Pearson correlation coefficient between
the predictions of two randomized trees built on the same learning set.

\end{frame}


\begin{frame}{Diagnosing the generalization error of random forests}

\begin{itemize}
\item Bias: {\color{blue} Identical} to the bias of a single randomized tree.
\item Variance: $\text{var}(\mathbf{x}) = \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x}) + \frac{1 - \rho(\mathbf{x})}{M} \sigma^2_{{\cal L},\theta}(\mathbf{x})$\\
As $M \to \infty$, {\color{red} $\text{var}(\mathbf{x}) \to \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x})$}
  \begin{itemize}
    \item The stronger the randomization, $\rho(\mathbf{x}) \to 0$, $\text{var}(\mathbf{x}) \to 0$.
    \item The weaker the randomization, $\rho(\mathbf{x}) \to 1$, $\text{var}(\mathbf{x}) \to \sigma^2_{{\cal L},\theta}(\mathbf{x})$
  \end{itemize}
\end{itemize}

\vspace{1cm}

{\bf Bias-variance trade-off.} Randomization increases bias but makes it
possible to reduce the variance of the corresponding ensemble model. The crux
of the problem is to {\color{red} find the right trade-off}.

\end{frame}

\begin{frame}{Back to our example}

\begin{table}
    \centering
    \begin{tabular}{| l c c |}
    \hline
        \textit{Method} & \textit{Trees} & \textit{MSE}  \\
    \hline
    \hline
    CART & 1 & {\color{red}1.055} \\
    Random Forest & 50 & {\color{blue}0.517} \\
    Extra-Trees & 50 & {\color{blue}0.507} \\
    \hline
    \end{tabular}
\end{table}

\vspace{0.5cm}

\begin{center}
Combining several randomized trees indeed works better!
\end{center}

\end{frame}



% Bias-variance ===============================================================

\section{Interpreting random forests}


\begin{frame}{Variable importances}

\begin{figure}
    \includegraphics[scale=0.3]{./figures/blackbox.jpg}
\end{figure}

\begin{itemize}
\item Interpretability can be recovered through {\color{blue}variable importances}

\bigskip

\item Two main importance measures:
    \begin{itemize}
    \item {\bf The mean decrease of impurity (MDI)}: summing total impurity reductions
      at all tree nodes where the variable appears {\scriptsize (Breiman et al., 1984)};
    \item {\bf The mean decrease of accuracy (MDA)}: measuring
      accuracy reduction on out-of-bag samples when the values of the
      variable are randomly permuted {\scriptsize (Breiman, 2001)}.
    \end{itemize}

\bigskip

\item We focus here on MDI because:
\begin{itemize}
\item It is faster to compute;
\item It does not require to use bootstrap sampling;
\item In practice, it correlates well with the MDA
  measure.
\end{itemize}

\end{itemize}
\end{frame}

\begin{frame}{Mean decrease of impurity}

\begin{figure}
    \includegraphics[scale=0.4]{./figures/mdi.pdf}
\end{figure}

Importance of variable $X_j$ for an ensemble of $M$ trees $\varphi_{m}$ is:
\begin{equation*}
\text{Imp}(X_j) = \frac{1}{M} \sum_{m=1}^M \sum_{t \in \varphi_{m}} 1(j_t = j) \Big[ p(t) \Delta i(t) \Big],
\end{equation*}
where $j_t$ denotes the variable used at node $t$, $p(t)=N_t/N$ and $\Delta i(t)$ is the impurity reduction at node $t$:
\begin{equation*}
\Delta i(t) = i(t) - \frac{N_{t_L}}{N_t} i(t_L) - \frac{N_{t_r}}{N_t} i(t_R)
\end{equation*}

\end{frame}

\begin{frame}{Back to our example}

MDI scores as computed from a forest of 1000 fully developed trees on the  Wine
dataset (Random Forest, default parameters).

\begin{figure}
    \includegraphics[scale=0.7]{./figures/imp-wine.pdf}
\end{figure}

\end{frame}

\begin{frame}{What does it mean?}
\begin{itemize}
\item MDI works well, but it is not well understood theoretically;
\item We would like to better characterize it and derive its main
  properties from this characterization.

\bigskip

\item Working assumptions:
\begin{itemize}
\item All variables are discrete;
\item Multi-way splits \`a la C4.5 (i.e., one branch per value);
\item Shannon entropy as impurity measure:
$$i(t) = - \sum_{c} \frac{N_{t,c}}{N_t} \log \frac{N_{t,c}}{N_t}$$
\item {\color{red} Totally randomized trees (RF with $K=1$);}
\item {\color{red} Asymptotic conditions: $N\to \infty$, $M\to \infty$.}
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Result 1: Three-level decomposition {\scriptsize (Louppe et al., 2013)}}

\hspace*{0.5cm}

{\bf Theorem.} Variable importances provide a three-level decomposition of the
information jointly provided by all the input variables about the output,
accounting for all interaction terms in a fair and exhaustive way.

\vspace{-0.5cm}

\begin{flalign*}
\underbrace{I(X_{1}, \ldots, X_{p} ; Y)}_{\substack{\text{Information jointly provided}\\
                                                                   \text{by all input variables}\\
                                                                   \text{about the output}}} =
\underbrace{\sum_{j=1}^{p}\text{Imp}(X_j)}_{\color{mygreen}\substack{\text{i) Decomposition in terms of}\\
                                                    \text{the MDI importance of}\\
                                                    \text{each input variable}}}\\
\end{flalign*}

\vspace{-1.5cm}

\begin{flalign*}
\text{Imp}(X_j) = \underbrace{\sum_{k=0}^{p-1} \frac{1}{C_p^k} \frac{1}{p-k}}_{{\color{blue} \substack{\text{ii) Decomposition along}\\
                                                                                                 \text{the degrees $k$ of interaction}\\
                                                                                                 \text{with the other variables}}}}
       \underbrace{\sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)}_{{\color{red} \substack{\text{iii) Decomposition along all}\\
                                                                                            \text{interaction terms $B$}\\
                                                                                            \text{of a given degree $k$}}}}\\
\end{flalign*}

\vspace{-1cm}

{\scriptsize E.g.:~$p=3, \text{Imp}(X_1) = \frac{1}{3} I(X_1;Y)+\frac{1}{6}(I(X_1;Y|X_2)+I(X_1;Y|X_3))+ \frac{1}{3} I(X_1;Y|X_2,X_3)$}

\end{frame}

\begin{frame}{Illustration: 7-segment display {\scriptsize (Breiman et al., 1984)}}

\only<1>{
\begin{columns}
\begin{column}{0.3\textwidth}
\begin{figure}
    \vspace{-0.5cm}
    \includegraphics[scale=0.35]{./figures/led-fig.png}
\end{figure}
\end{column}
\begin{column}{0.7\textwidth}
\begin{center}
    \begin{tabular}{| c | c c c c c c c |}
    \hline
    $y$ & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$ \\
    \hline
    \hline
    0 & 1 & 1 & 1 & 0 & 1 & 1 & 1 \\
    1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
    2 & 1 & 0 & 1 & 1 & 1 & 0 & 1 \\
    3 & 1 & 0 & 1 & 1 & 0 & 1 & 1 \\
    4 & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\
    5 & 1 & 1 & 0 & 1 & 0 & 1 & 1 \\
    6 & 1 & 1 & 0 & 1 & 1 & 1 & 1 \\
    7 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\
    8 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    9 & 1 & 1 & 1 & 1 & 0 & 1 & 1 \\
    \hline
    \end{tabular}
\end{center}
\end{column}
\end{columns}
}

\only<2>{
\begin{equation*}
\text{Imp}(X_j)=\sum_{k=0}^{p-1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)
\end{equation*}
  \begin{columns}[t]
\begin{column}{0.3\textwidth}
\begin{center}
    \begin{tabular}{| c | c |}
    \hline
        Var & Imp \\
    \hline
    \hline
    $X_1$ & 0.412 \\
    $X_2$ & 0.581 \\
    $X_3$ & 0.531 \\
    $X_4$ & 0.542 \\
    $X_5$ & 0.656 \\
    $X_6$ & 0.225 \\
    $X_7$ & 0.372 \\
    \hline
    \hline
    $\sum$& 3.321 \\
    \hline
    \end{tabular}
\end{center}
\end{column}
\begin{column}{0.7\textwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/led-imp.pdf}\\
\hspace{0.5cm}$k$
\end{center}
\end{column}
\end{columns}
}

\end{frame}

\begin{frame}{Result 2: Irrelevant variables {\scriptsize (Louppe et al., 2013)}}

{\bf Theorem.} Variable importances depend only on the relevant variables.

\vspace{1cm}

{\bf Theorem.} A variable $X_j$ is irrelevant if and only if $\text{Imp}(X_j)= 0$.

\vspace{1cm}

{\color{blue} $\Rightarrow$ The importance of a relevant variable is
insensitive  to the addition or the removal of irrelevant variables.}

\vspace{1cm}


{\scriptsize \textbf{Definition} {\scriptsize (Kohavi \& John, 1997)}. A variable $X$ is
{\it irrelevant} (to $Y$ with respect to $V$) if, for all $B\subseteq V$,
$I(X;Y|B)=0$. A variable is {\it relevant} if it is not irrelevant.}

\end{frame}

\begin{frame}{Relaxing assumptions}

\begin{block}{When trees are not totally random...}

\begin{itemize}
\item  There can be {\color{red} relevant variables with zero
importances} (due to masking effects).
\item   The importance of relevant variables can be
{\color{red} influenced by the number of irrelevant variables}.
\end{itemize}

\end{block}

\begin{block}{When the learning set is finite...}
\begin{itemize}
\item  Importances are {\color{red} biased towards variables of  high
cardinality}.

\item This effect can be minimized by collecting impurity
terms measured from large enough sample only.
\end{itemize}
\end{block}

\begin{block}{When splits are not multiway...}
\begin{itemize}
\item $i(t)$ does not actually measure the mutual information.
\end{itemize}
\end{block}

\end{frame}


\begin{frame}{Back to our example}

MDI scores as computed from a forest of 1000 fixed-depth trees on the  Wine
dataset (Extra-Trees, $K=1$, $\texttt{max\_depth}=5$).

\begin{figure}
    \includegraphics[scale=0.7]{./figures/imp-wine2.pdf}
\end{figure}

\begin{center}
Taking into account (some of) the biases\\
results in {\color{red} quite a different story}!
\end{center}

\end{frame}


% Computational performance ===================================================

\section{Implementing and accelerating random forests}

\begin{frame}{Implementation {\scriptsize (Buitinck et al., 2013)}}

\begin{columns}
\begin{column}[t]{0.7\textwidth}
\begin{block}{Scikit-Learn}
\begin{itemize}
    \item Open source {\bf machine learning} library for Python
    \item Classical and {\bf well-established algorithms}
    \item Emphasis on {\bf code quality} and {\bf usability}
\end{itemize}
\end{block}
\end{column}
\begin{column}[t]{0.20\textwidth}
    \begin{figure}
    \includegraphics[scale=0.35]{./figures/scikit-learn-logo.pdf}
    \end{figure}
\end{column}
\end{columns}

\begin{block}{A long team effort}
\begin{figure}
\vspace{-0.25cm}
{\small \it Time for building a Random Forest (relative to version \texttt{0.10})}\\[1ex]
  \includegraphics[scale=0.5]{./figures/fit-time.pdf}
\end{figure}
\end{block}

\end{frame}

\begin{frame}[fragile]{Implementation overview}

\begin{itemize}
\item Modular implementation, designed with a strict {\bf separation of concerns}
  \begin{itemize}
    \item Builders: for building and connecting nodes into a tree
    \item Splitters: for finding a split
    \item Criteria: for evaluating the goodness of a split
    \item Tree: dedicated data structure
  \end{itemize}
\item Efficient {\bf algorithmic formulation} [See \href{http://arxiv.org/abs/1407.7502}{Louppe, 2014}]\\
  \begin{itemize}
    \item Dedicated sorting procedure
    \item Efficient evaluation of consecutive splits
  \end{itemize}
\item {\bf Close to the metal}, carefully coded, implementation\\
  {\small 2300+ lines of Python, 3000+ lines of Cython, 1700+ lines of tests}

\end{itemize}

\vspace{0.1cm}

\begin{minted}[fontsize=\footnotesize]{python}
# But we kept it stupid simple for users!
clf = RandomForestClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
\end{minted}

\end{frame}

\begin{frame}{A winning strategy}
Scikit-Learn implementation proves to be {\bf one of the fastest} among all libraries and programming languages.
  \begin{figure}
  \includegraphics[scale=0.5]{./figures/bench.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Computational complexity {\scriptsize (Louppe, 2014)}}

\begin{table}
    \centering
    \begin{tabular}{| c | c |}
    \hline
         & \textit{Average time complexity}  \\
    \hline
    \hline
    CART & $\Theta(pN\log^2 N)$ \\
    Random Forest & $\Theta(MK\widetilde{N}\log^2 \widetilde{N})$  \\
    Extra-Trees & $\Theta(MKN\log N)$  \\
    \hline
    \end{tabular}
\end{table}

\begin{center}
    \begin{itemize}
    \item $N$: number of samples in ${\cal L}$
    \item $p$: number of input variables
    \item $K$: the number of variables randomly drawn at each node
    \item $\widetilde{N} = 0.632 N$.
    \end{itemize}
\end{center}

\end{frame}


\begin{frame}{Improving scalability through randomization}

\begin{block}{Motivation}
\begin{itemize}
\item Randomization and averaging allow to improve accuracy by reducing variance.
\item As a nice side-effect, the resulting algorithms are fast and
  embarrassingly parallel.
\item Why not purposely exploit randomization to make the algorithm
  even more scalable (and at least as accurate)?
\end{itemize}
\end{block}

\begin{block}{Problem}
\begin{itemize}
\item Let assume a supervised learning problem of $N_s$ samples defined over $N_f$ features.
Let also assume $T$ computing nodes, each with a
  memory capacity limited to $M_{max}$, with $M_{max}\ll N_s \times N_f$.
\item How to best exploit the memory constraint to obtain the most accurate model, as quickly as possible?
\end{itemize}
\end{block}

\end{frame}



\begin{frame}{A straightforward solution: Random Patches {\scriptsize (Louppe et al., 2012)}}

\begin{columns}[t]
\begin{column}[t]{0.4\textwidth}

\begin{figure}
\vspace{-1cm}
\includegraphics[scale=0.35]{figures/rp-1.pdf}
\end{figure}

\end{column}

\begin{column}{0.6\textwidth}

\begin{enumerate}
\item Draw a subsample $r$ of $p_s N_s$ random examples, with $p_f N_f$ random features.
\item Build a {\it base estimator} on $r$.
\item Repeat 1-2 for a number $T$ of estimators.
\item Aggregate the predictions by voting.
\end{enumerate}

\bigskip

{\small
$p_s$ and $p_f$ are two meta-parameters that should be selected
\begin{itemize}
\item such that $p_sN_s\times p_fN_f\leq M_{max}$
\item to optimize accuracy
\end{itemize}
}

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Impact of memory constraint}

\begin{center}
\includegraphics[width=10cm]{figures/rp-memory.pdf}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Lessons learned from subsampling}

\begin{itemize}

\item \textcolor{red}{Training each estimator on the whole data is (often)
useless}. The size of the random patches can be reduced without (significant)
loss in accuracy.

\item As a result, \textcolor{blue}{both memory consumption and training time
can be reduced}, at low cost.

\item With strong memory constraints, RP can exploit data better than the
other methods.

\item \textcolor{blue}{Sampling features is critical to improve accuracy.}
Sampling the examples only is often ineffective.
\end{itemize}

\end{frame}


% Conclusions =================================================================

\section{Conclusions}

\begin{frame}{Opening the black box}
\begin{itemize}

\item Random forests constitute one of the most {\color{blue} robust and
effective} machine learning algorithms for many problems.

\medskip

\item While simple in design and easy to use, random forests remain however
  \begin{itemize}
    {\color{red}
    \item hard to analyze theoretically,
    \item non-trivial to interpret,
    \item difficult to implement properly.}
  \end{itemize}

\medskip

\item Through an in-depth re-assessment of the method, this dissertation has
proposed {\color{blue} original contributions} on these issues.

\end{itemize}

\begin{center}
\includegraphics[scale=0.5]{figures/blackbox-open.jpg}
\end{center}

\end{frame}

\begin{frame}{Future works}

\begin{block}{Variable importances}
\begin{itemize}
\item Theoretical characterization of variable importances in a finite setting.
\item (Re-analysis of) empirical studies based on variable importances, in light
of the results and conclusions of the thesis.
\item Study of variable importances in boosting.
\end{itemize}
\end{block}

\begin{block}{Subsampling}
\begin{itemize}
\item Finer study of subsampling statistical mechanisms.
\item Smart sampling.
\end{itemize}
\end{block}

\end{frame}


\appendix

\begin{frame}
\begin{center}
{\Huge  Questions?}
\end{center}
\end{frame}


\begin{frame}

\begin{center}
{\it Backup slides}
\end{center}

\end{frame}

\begin{frame}{Condorcet's jury theorem}

\begin{columns}
\begin{column}{0.65\textwidth}
Let consider a group of $M$ voters.

\vspace{1cm}

If each voter has an independent  probability $p > \tfrac{1}{2}$ of voting for
the correct decision, then adding more voters increases the probability of the
majority decision to be correct.

\vspace{1cm}

When $M \to \infty$, the probability that the
decision taken by the group is correct approaches $1$.

\end{column}

\begin{column}{0.35\textwidth}
\begin{figure}
    \includegraphics[scale=1.0]{./figures/condorcet.png}
\end{figure}
\end{column}
\end{columns}

\end{frame}


\begin{frame}{Interpretation of $\rho(\mathbf{x})$ {\scriptsize (Louppe, 2014)}}

{\bf Theorem.} $\rho(\mathbf{x}) = \frac{\mathbb{V}_{\cal L} \{ \mathbb{E}_{\theta|{\cal L}} \{ \varphi_{{\cal L},\theta}(\mathbf{x}) \} \}}{\mathbb{V}_{\cal L} \{ \mathbb{E}_{\theta|{\cal L}} \{ \varphi_{{\cal L},\theta}(\mathbf{x}) \} \} + \mathbb{E}_{\cal L} \{ \mathbb{V}_{\theta|{\cal L}} \{ \varphi_{{\cal L},\theta}(\mathbf{x}) \} \}}$

\vspace{1cm}

In other words, it is the ratio between
\begin{itemize}
\item the variance due to the learning set and
\item the total variance, accounting for random effects due to both the
  learning set and the random perburbations.
\end{itemize}

\bigskip

$\rho(\mathbf{x}) \to 1$ when variance is mostly due to the learning set; \\
$\rho(\mathbf{x}) \to 0$ when variance is mostly due to the random perturbations;\\
$\rho(\mathbf{x}) \geq 0$.


\end{frame}


\end{document}
